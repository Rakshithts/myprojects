{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvlqStBUtqdwqWFlf6j5Ri",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakshithts/myprojects/blob/main/fine_tuned_model_for_news_format_sentence_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets peft accelerate -q"
      ],
      "metadata": {
        "id": "wl5p8J3N6GiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import os\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "CmLu9az38diK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable W&B logging\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "RJ2qzXXT8dkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load a larger dataset (AG News)\n",
        "raw_dataset = load_dataset(\"ag_news\")\n",
        "train_texts = raw_dataset[\"train\"][\"text\"][:20000]\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": train_texts})"
      ],
      "metadata": {
        "id": "px214m4j8doB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load tokenizer & model\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "j16TwaMe8dqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Tokenize with labels\n",
        "def tokenize(batch):\n",
        "    tokenized = tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "tokenized_ds = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "id": "eNBCEoGn8dt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Apply LoRA (PEFT)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 attention layers\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "jAqeY-Oi8dwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Training setup\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,   # bigger batch size for speed\n",
        "    learning_rate=5e-4,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        "    save_strategy=\"no\",\n",
        "    fp16=torch.cuda.is_available()   # mixed precision if GPU supports it\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds\n",
        ")\n"
      ],
      "metadata": {
        "id": "szrKKCUj8dz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Train\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "K3QEPBQh9Llm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Test generation\n",
        "prompt = \"The weather today\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=30, do_sample=True, top_k=40, top_p=0.9)\n",
        "print(\"\\nGenerated text:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "qmLJXZTHHYO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Test generation\n",
        "prompt = \"The movie kept the audience\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=30, do_sample=True, top_k=40, top_p=0.9)\n",
        "print(\"\\nGenerated text:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "HVIk6czsqfGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Function for generating text\n",
        "def generate_text(model, tokenizer, prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=30,\n",
        "        do_sample=True,\n",
        "        top_k=40,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "prompt = \"The politics\"\n",
        "\n",
        "# 1. Load base DistilGPT-2 for BEFORE result\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(model.device)\n",
        "base_model.eval()\n",
        "\n",
        "print(\"=== BEFORE Fine-Tuning ===\")\n",
        "before_text = generate_text(base_model, tokenizer, prompt)\n",
        "print(before_text)\n",
        "\n",
        "# 2. AFTER: Fine-tuned model (already trained above)\n",
        "print(\"\\n=== AFTER Fine-Tuning ===\")\n",
        "after_text = generate_text(model, tokenizer, prompt)\n",
        "print(after_text)\n"
      ],
      "metadata": {
        "id": "pio6x3zS6_m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ho4sJFe8E_3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}