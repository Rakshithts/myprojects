{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrnMUmVZ4LSAL+MjJvHyHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakshithts/myprojects/blob/main/NLP_assignment_bbc_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install pyLDAvis\n",
        "!pip install bertopic umap-learn hdbscan\n"
      ],
      "metadata": {
        "id": "6Z55jG9WTW1C",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and Preview Data\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/bbc-text.csv\")\n",
        "\n",
        "# Preview\n",
        "print(df.head())\n",
        "print(df['category'].value_counts())\n"
      ],
      "metadata": {
        "id": "VlQ-Dy3PXGnP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W|\\d', ' ', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['clean_text'] = df['text'].apply(preprocess)\n",
        "tokenized_docs = [doc.split() for doc in df['clean_text']]\n"
      ],
      "metadata": {
        "id": "L8IdrgNhXGq4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LDA Topic Modeling (Gensim)\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "dictionary = corpora.Dictionary(tokenized_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=42, passes=10)\n",
        "lda_model.print_topics()\n"
      ],
      "metadata": {
        "id": "_GZWhGxOXGtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(vis_data)\n"
      ],
      "metadata": {
        "id": "7NNPggdqUfUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSA Topic Modeling (TruncatedSVD)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
        "X_tfidf = vectorizer.fit_transform(df['clean_text'])\n",
        "\n",
        "lsa_model = TruncatedSVD(n_components=5, random_state=42)\n",
        "lsa_topic_matrix = lsa_model.fit_transform(X_tfidf)\n",
        "\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for i, comp in enumerate(lsa_model.components_):\n",
        "    sorted_terms = sorted(zip(terms, comp), key=lambda x: x[1], reverse=True)[:10]\n",
        "    print(f\"LSA Topic {i}: {[t[0] for t in sorted_terms]}\")\n"
      ],
      "metadata": {
        "id": "jDiVos2sXGwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# LDA\n",
        "for i in range(5):\n",
        "    plt.figure()\n",
        "    plt.imshow(WordCloud(background_color='white').fit_words(dict(lda_model.show_topic(i, 20))))\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"LDA Topic {i}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "597tjVYNXGzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "topic_dist = [lda_model.get_document_topics(corpus[i]) for i in range(10)]\n",
        "matrix = np.zeros((10, 5))\n",
        "for i, dist in enumerate(topic_dist):\n",
        "    for topic_id, prob in dist:\n",
        "        matrix[i, topic_id] = prob\n",
        "\n",
        "sns.heatmap(matrix, annot=True, cmap=\"YlGnBu\")\n",
        "plt.xlabel(\"Topics\")\n",
        "plt.ylabel(\"Documents\")\n",
        "plt.title(\"Topic Distribution in First 10 Docs\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2j2m1CsOgh6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERTopic Modeling\n",
        "\n",
        "\n",
        "from bertopic import BERTopic\n",
        "\n",
        "docs = df['clean_text'].tolist()\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
        "topics, probs = topic_model.fit_transform(docs)\n",
        "\n",
        "topic_model.get_topic_info().head()\n"
      ],
      "metadata": {
        "id": "QiLdP_C_UqZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "\n",
        "# Simulate timestamps if not available\n",
        "df['date'] = pd.date_range(start='2021-01-01', periods=len(df), freq='D')"
      ],
      "metadata": {
        "id": "P3cuwpIOVuOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics_over_time = topic_model.topics_over_time(docs, df['date'])\n",
        "topic_model.visualize_topics_over_time(topics_over_time)\n",
        "\n",
        "df['dominant_topic'] = topics\n",
        "df['topic_name'] = df['dominant_topic'].apply(lambda t: topic_model.get_topic(t)[0][0] if t != -1 else \"No topic\")"
      ],
      "metadata": {
        "id": "C_C-be2kXap8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coherence Score for LDA and LSA\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models import LsiModel\n",
        "\n",
        "# LDA\n",
        "coherence_lda = CoherenceModel(model=lda_model, texts=tokenized_docs, dictionary=dictionary, coherence='c_v')\n",
        "# print(\"LDA Coherence Score:\", coherence_lda.get_coherence())\n",
        "\n",
        "# LSA (LSI)\n",
        "lsi_model = LsiModel(corpus=corpus, id2word=dictionary, num_topics=5)\n",
        "coherence_lsa = CoherenceModel(model=lsi_model, texts=tokenized_docs, dictionary=dictionary, coherence='c_v')\n",
        "# print(\"LSA Coherence Score:\", coherence_lsa.get_coherence())\n",
        "\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "# Prepare BERTopic topics in gensim format\n",
        "# Get topic-word mapping for all topics (excluding -1 which means no topic assigned)\n",
        "bertopic_topics = topic_model.get_topics()\n",
        "valid_topic_ids = [t for t in bertopic_topics.keys() if t != -1]\n",
        "\n",
        "# Convert to list of words per topic\n",
        "bertopic_topic_words = []\n",
        "for topic_id in valid_topic_ids:\n",
        "    words = [word for word, _ in bertopic_topics[topic_id]]\n",
        "    bertopic_topic_words.append(words)\n",
        "\n",
        "# Compute coherence\n",
        "bertopic_coherence_model = CoherenceModel(\n",
        "    topics=bertopic_topic_words,\n",
        "    texts=tokenized_docs,  # from earlier preprocessing\n",
        "    dictionary=dictionary,  # from LDA prep\n",
        "    coherence='c_v'\n",
        ")\n",
        "\n",
        "# print(\"BERTopic Coherence Score:\", bertopic_coherence_model.get_coherence())\n",
        "\n"
      ],
      "metadata": {
        "id": "g3YRPSL7Yg24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model Coherence Scores:\")\n",
        "print(f\"✅ LDA Coherence:      {coherence_lda.get_coherence():.4f}\")\n",
        "print(f\"✅ LSA Coherence:      {coherence_lsa.get_coherence():.4f}\")\n",
        "print(f\"✅ BERTopic Coherence: {bertopic_coherence_model.get_coherence():.4f}\")\n"
      ],
      "metadata": {
        "id": "50pL17BFYubV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "topic_summaries = {}\n",
        "\n",
        "for topic_id in df['dominant_topic'].unique():\n",
        "    topic_docs = df[df['dominant_topic'] == topic_id]['text'].tolist()\n",
        "    combined_text = \" \".join(topic_docs[:5])\n",
        "    if len(combined_text) > 1000:\n",
        "        combined_text = combined_text[:1000]\n",
        "    summary = summarizer(combined_text, max_length=120, min_length=40, do_sample=False)\n",
        "    topic_summaries[topic_id] = summary[0]['summary_text']\n",
        "    print(f\"Topic {topic_id} Summary:\\n{summary[0]['summary_text']}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Xb6auMkxXcOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFSwGrEYX2Nv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}